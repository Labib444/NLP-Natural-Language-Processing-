{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "encoder_decoder_with_attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKLIWVSwQVut"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "import os, sys\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size for training.\n",
        "EPOCHS = 100  # Number of epochs to train for.\n",
        "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\n",
        "NUM_SAMPLES = 10000  # Number of samples to train on.\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "input_texts = [] # sentence in original language\n",
        "target_texts = [] # sentence in target language\n",
        "target_texts_inputs = [] # sentence in target language offset by 1\n",
        "\n",
        "t = 0\n",
        "for line in open('/content/drive/My Drive/NLP/ben.txt'):\n",
        "  # only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "\n",
        "  # input and target are separated by tab\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input and translation\n",
        "  input_text, translation, attrib = line.rstrip().split('\\t')\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + ' <eos>'\n",
        "  target_text_input = '<sos> ' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "  print(\"num samples:\", len(input_texts))\n",
        "\n",
        "\n",
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)\n",
        "\n",
        "# tokenize the outputs\n",
        "# don't filter out special characters\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) #inefficient, oh well\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
        "\n",
        "# store number of output words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)\n",
        "\n",
        "# pad the sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
        "\n",
        "\n",
        "# store all the pre-trained word vectors\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with open(os.path.join('/content/drive/My Drive/NLP/glove.6B.100d.txt')) as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))\n",
        "\n",
        "\n",
        "\n",
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# embedding_matrix_decoder = np.zeros((num_words_output, EMBEDDING_DIM))\n",
        "# for word, i in word2idx_outputs.items():\n",
        "#   if i < MAX_NUM_WORDS:\n",
        "#     embedding_vector = word2vec.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#       # words not found in embedding index will be all zeros.\n",
        "#       embedding_matrix_decoder[i] = embedding_vector\n",
        "\n",
        "\n",
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len_input,\n",
        "  # trainable=True\n",
        ")\n",
        "\n",
        "\n",
        "# create targets, since we cannot use sparse\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "  (\n",
        "    len(input_texts),\n",
        "    max_len_target,\n",
        "    num_words_output\n",
        "  ),\n",
        "  dtype='float32'\n",
        ")\n",
        "\n",
        "# assign the values\n",
        "for i, d in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(d):\n",
        "    decoder_targets_one_hot[i, t, word] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### build the model #####\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = LSTM(LATENT_DIM, return_state=True, dropout=0.2)\n",
        "encoder_outputs, h, c = encoder(x)\n",
        "# encoder_outputs, h = encoder(x) #gru\n",
        "\n",
        "# keep only the states to pass into decoder\n",
        "encoder_states = [h, c]\n",
        "# encoder_states = [state_h] # gru\n",
        "\n",
        "# Set up the decoder, using [h, c] as initial state.\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "#decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM, weights=[embedding_matrix_decoder], input_length=max_len_target)\n",
        "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n",
        "# since the decoder is a \"to-many\" model we want to have\n",
        "# return_sequences=True\n",
        "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.5)\n",
        "decoder_outputs, _, _ = decoder_lstm(\n",
        "  decoder_inputs_x,\n",
        "  initial_state=encoder_states\n",
        ")\n",
        "\n",
        "# decoder_outputs, _ = decoder_gru(\n",
        "#   decoder_inputs_x,\n",
        "#   initial_state=encoder_states\n",
        "# )\n",
        "\n",
        "# final dense layer for predictions\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Create the model object\n",
        "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)\n",
        "\n",
        "# Compile the model and train it\n",
        "model.compile(\n",
        "  optimizer='rmsprop',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "r = model.fit(\n",
        "  [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        ")\n",
        "\n",
        "# # plot some data\n",
        "# plt.plot(r.history['loss'], label='loss')\n",
        "# plt.plot(r.history['val_loss'], label='val_loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # accuracies\n",
        "# plt.plot(r.history['acc'], label='acc')\n",
        "# plt.plot(r.history['val_acc'], label='val_acc')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # Save model\n",
        "# model.save('s2s.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAFJD2MngexB"
      },
      "source": [
        "##### Make predictions #####\n",
        "# The encoder will be stand-alone\n",
        "# From this we will get our initial decoder hidden state\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
        "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# decoder_states_inputs = [decoder_state_input_h] # gru\n",
        "\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "\n",
        "# this time, we want to keep the states too, to be output\n",
        "# by our sampling model\n",
        "decoder_outputs, h, c = decoder_lstm(\n",
        "  decoder_inputs_single_x,\n",
        "  initial_state=decoder_states_inputs\n",
        ")\n",
        "# decoder_outputs, state_h = decoder_lstm(\n",
        "#   decoder_inputs_single_x,\n",
        "#   initial_state=decoder_states_inputs\n",
        "# ) #gru\n",
        "decoder_states = [h, c]\n",
        "# decoder_states = [h] # gru\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# The sampling model\n",
        "# inputs: y(t-1), h(t-1), c(t-1)\n",
        "# outputs: y(t), h(t), c(t)\n",
        "decoder_model = Model(\n",
        "  [decoder_inputs_single] + decoder_states_inputs, \n",
        "  [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    output_tokens, h, c = decoder_model.predict(\n",
        "      [target_seq] + states_value\n",
        "    )\n",
        "    # output_tokens, h = decoder_model.predict(\n",
        "    #     [target_seq] + states_value\n",
        "    # ) # gru\n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "    # states_value = [h] # gru\n",
        "\n",
        "  return ' '.join(output_sentence)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqWYj-Azgs6F"
      },
      "source": [
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(input_texts))\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input:', input_texts[i])\n",
        "  print('Translation:', translation)\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyFdtFLihAwF",
        "outputId": "67276764-720e-4b49-94ed-c73745b26b78"
      },
      "source": [
        "string = 'I wish i can fly'\n",
        "string = string.lower()\n",
        "jam = []\n",
        "for i in string.split():\n",
        "  jam.append( word2idx_inputs[i] )\n",
        "jam = np.reshape( jam, (1, len(jam) ) )\n",
        "jam.shape\n",
        "jam_pad = pad_sequences(jam, maxlen=max_len_input)\n",
        "\n",
        "#translation = decode_sequence(input_seq)\n",
        "translation = decode_sequence(jam_pad)\n",
        "print('-')\n",
        "print('Input sentence:', string)\n",
        "print('Predicted translation:', translation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: i wish i can fly\n",
            "Predicted translation: আমি যদি খেতে ভালোবাসি।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JD9KQxIgyZG",
        "outputId": "04ef6fa3-d612-4003-f560-7c66424bff39"
      },
      "source": [
        "string = 'I am not eating'\n",
        "string = string.lower()\n",
        "jam = []\n",
        "for i in string.split():\n",
        "  jam.append( word2idx_inputs[i] )\n",
        "jam = np.reshape( jam, (1, len(jam) ) )\n",
        "jam.shape\n",
        "jam_pad = pad_sequences(jam, maxlen=max_len_input)\n",
        "\n",
        "#translation = decode_sequence(input_seq)\n",
        "translation = decode_sequence(jam_pad)\n",
        "print('-')\n",
        "print('Input sentence:', string)\n",
        "print('Predicted translation:', translation)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: i am not eating\n",
            "Predicted translation: আমি ভাল নই।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vupp6Qmcg_ov"
      },
      "source": [
        "**Encoder Decoder with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DK0-unfRqu6",
        "outputId": "87ac62f6-bab4-4046-8702-8478277eb53f"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "import os, sys\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
        "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# make sure we do softmax over the time axis\n",
        "# expected shape is N x T x D\n",
        "# note: the latest version of Keras allows you to pass in axis arg\n",
        "def softmax_over_time(x):\n",
        "  assert(K.ndim(x) > 2)\n",
        "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
        "  s = K.sum(e, axis=1, keepdims=True)\n",
        "  return e / s\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  # both are of shape N x T x K\n",
        "  mask = K.cast(y_true > 0, dtype='float32')\n",
        "  out = mask * y_true * K.log(y_pred)\n",
        "  return -K.sum(out) / K.sum(mask)\n",
        "\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "  # both are of shape N x T x K\n",
        "  targ = K.argmax(y_true, axis=-1)\n",
        "  pred = K.argmax(y_pred, axis=-1)\n",
        "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
        "\n",
        "  # 0 is padding, don't include those\n",
        "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
        "  n_correct = K.sum(mask * correct)\n",
        "  n_total = K.sum(mask)\n",
        "  return n_correct / n_total\n",
        "\n",
        "# config\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LATENT_DIM = 256\n",
        "LATENT_DIM_DECODER = 256 # idea: make it different to ensure things all fit together properly!\n",
        "NUM_SAMPLES = 10000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "input_texts = [] # sentence in original language\n",
        "target_texts = [] # sentence in target language\n",
        "target_texts_inputs = [] # sentence in target language offset by 1\n",
        "\n",
        "\n",
        "# download the data at: http://www.manythings.org/anki/\n",
        "t = 0\n",
        "for line in open('/content/drive/My Drive/NLP/ben.txt'):\n",
        "  # only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "  # input and target are separated by tab\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input and translation\n",
        "  input_text, translation, attrib = line.rstrip().split('\\t')\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + ' <eos>'\n",
        "  target_text_input = '<sos> ' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "print(\"num samples:\", len(input_texts))\n",
        "\n",
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)\n",
        "\n",
        "# tokenize the outputs\n",
        "# don't filter out special characters\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
        "\n",
        "# store number of output words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples: 4350\n",
            "Found 1874 unique input tokens.\n",
            "Found 3542 unique output tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzbgQ56fg5Ug",
        "outputId": "50c28636-5a0b-471a-896c-7885f63b10f2"
      },
      "source": [
        "# pad the sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
        "\n",
        "# store all the pre-trained word vectors\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with open(os.path.join('/content/drive/My Drive/NLP/glove.6B.100d.txt')) as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))\n",
        "\n",
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len_input,\n",
        "  # trainable=True\n",
        ")\n",
        "\n",
        "\n",
        "# create targets, since we cannot use sparse\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "  (\n",
        "    len(input_texts),\n",
        "    max_len_target,\n",
        "    num_words_output\n",
        "  ),\n",
        "  dtype='float32'\n",
        ")\n",
        "\n",
        "# assign the values\n",
        "for i, d in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(d):\n",
        "    #if word > 0:\n",
        "    decoder_targets_one_hot[i, t, word] = 1\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_data.shape: (4350, 19)\n",
            "encoder_data[0]: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28]\n",
            "decoder_data[0]: [  2 167   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0]\n",
            "decoder_data.shape: (4350, 19)\n",
            "Loading word vectors...\n",
            "Found 400000 word vectors.\n",
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc1p63-IgvRI",
        "outputId": "e8d03c04-9a2b-43da-ead6-6cdfda81b1db"
      },
      "source": [
        "##### build the model #####\n",
        "# Set up the encoder\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, dropout=0.2))\n",
        "encoder_outputs = encoder(x)\n",
        "\n",
        "\n",
        "# Set up the decoder\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n",
        "\n",
        "######### Attention #########\n",
        "# Attention layers need to be global because\n",
        "# they will be repeated Ty times at the decoder\n",
        "attn_repeat_layer = RepeatVector(max_len_input)\n",
        "attn_concat_layer = Concatenate(axis=-1)\n",
        "attn_dense1 = Dense(10, activation='tanh')\n",
        "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
        "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
        "\n",
        "def one_step_attention(h, st_1):\n",
        "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
        "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
        "\n",
        "  # copy s(t-1) Tx times\n",
        "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
        "  st_1 = attn_repeat_layer(st_1)\n",
        "\n",
        "  # Concatenate all h(t)'s with s(t-1)\n",
        "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
        "  x = attn_concat_layer([h, st_1])\n",
        "\n",
        "  # Neural net first layer\n",
        "  x = attn_dense1(x)\n",
        "\n",
        "  # Neural net second layer with special softmax over time\n",
        "  alphas = attn_dense2(x)\n",
        "\n",
        "  # \"Dot\" the alphas and the h's\n",
        "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
        "  context = attn_dot([alphas, h])\n",
        "\n",
        "  return context\n",
        "\n",
        "\n",
        "# define the rest of the decoder (after attention)\n",
        "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "\n",
        "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
        "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
        "context_last_word_concat_layer = Concatenate(axis=2)\n",
        "\n",
        "\n",
        "# Unlike previous seq2seq, we cannot get the output\n",
        "# all in one step\n",
        "# Instead we need to do Ty steps\n",
        "# And in each of those steps, we need to consider\n",
        "# all Tx h's\n",
        "\n",
        "# s, c will be re-assigned in each iteration of the loop\n",
        "s = initial_s\n",
        "c = initial_c\n",
        "\n",
        "# collect outputs in a list at first\n",
        "outputs = []\n",
        "for t in range(max_len_target): # Ty times\n",
        "  # get the context using attention\n",
        "  context = one_step_attention(encoder_outputs, s)\n",
        "\n",
        "  # we need a different layer for each time step\n",
        "  selector = Lambda(lambda x: x[:, t:t+1])\n",
        "  xt = selector(decoder_inputs_x)\n",
        "\n",
        "  # combine \n",
        "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
        "\n",
        "  # pass the combined [context, last word] into the LSTM\n",
        "  # along with [s, c]\n",
        "  # get the new [s, c] and output\n",
        "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
        "\n",
        "  # final dense layer to get next word prediction\n",
        "  decoder_outputs = decoder_dense(o)\n",
        "  outputs.append(decoder_outputs)\n",
        "\n",
        "\n",
        "# 'outputs' is now a list of length Ty\n",
        "# each element is of shape (batch size, output vocab size)\n",
        "# therefore if we simply stack all the outputs into 1 tensor\n",
        "# it would be of shape T x N x D\n",
        "# we would like it to be of shape N x T x D\n",
        "def stack_and_transpose(x):\n",
        "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
        "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
        "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
        "  return x\n",
        "\n",
        "# make it a layer\n",
        "stacker = Lambda(stack_and_transpose)\n",
        "outputs = stacker(outputs)\n",
        "\n",
        "# create the model\n",
        "model = Model(\n",
        "  inputs=[\n",
        "    encoder_inputs_placeholder,\n",
        "    decoder_inputs_placeholder,\n",
        "    initial_s, \n",
        "    initial_c,\n",
        "  ],\n",
        "  outputs=outputs\n",
        ")\n",
        "# compile the model\n",
        "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
        "# train the model\n",
        "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
        "r = model.fit(\n",
        "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2\n",
        ")\n",
        "\n",
        "# # plot some data\n",
        "# plt.plot(r.history['loss'], label='loss')\n",
        "# plt.plot(r.history['val_loss'], label='val_loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # accuracies\n",
        "# plt.plot(r.history['acc'], label='acc')\n",
        "# plt.plot(r.history['val_acc'], label='val_acc')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "55/55 [==============================] - 12s 214ms/step - loss: 2.8039 - acc: 0.0000e+00 - val_loss: 3.2380 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 1.6244 - acc: 0.0195 - val_loss: 3.3289 - val_acc: 0.0236\n",
            "Epoch 3/100\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 1.5344 - acc: 0.0777 - val_loss: 3.0336 - val_acc: 0.0418\n",
            "Epoch 4/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.4787 - acc: 0.1187 - val_loss: 3.0836 - val_acc: 0.0499\n",
            "Epoch 5/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 1.4354 - acc: 0.1478 - val_loss: 3.0536 - val_acc: 0.0736\n",
            "Epoch 6/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.4015 - acc: 0.1658 - val_loss: 3.0291 - val_acc: 0.0929\n",
            "Epoch 7/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.3755 - acc: 0.1897 - val_loss: 3.1288 - val_acc: 0.0655\n",
            "Epoch 8/100\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 1.3488 - acc: 0.2002 - val_loss: 3.0502 - val_acc: 0.1147\n",
            "Epoch 9/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.3321 - acc: 0.2085 - val_loss: 3.2135 - val_acc: 0.0719\n",
            "Epoch 10/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.3165 - acc: 0.2116 - val_loss: 3.1782 - val_acc: 0.0860\n",
            "Epoch 11/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.3002 - acc: 0.2197 - val_loss: 3.1948 - val_acc: 0.0826\n",
            "Epoch 12/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.2866 - acc: 0.2255 - val_loss: 3.1348 - val_acc: 0.0986\n",
            "Epoch 13/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.2717 - acc: 0.2294 - val_loss: 3.1445 - val_acc: 0.0920\n",
            "Epoch 14/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 1.2554 - acc: 0.2348 - val_loss: 3.1255 - val_acc: 0.1030\n",
            "Epoch 15/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 1.2443 - acc: 0.2352 - val_loss: 3.1317 - val_acc: 0.0975\n",
            "Epoch 16/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 1.2313 - acc: 0.2382 - val_loss: 3.0977 - val_acc: 0.1170\n",
            "Epoch 17/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 1.2168 - acc: 0.2406 - val_loss: 3.1550 - val_acc: 0.0949\n",
            "Epoch 18/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.1974 - acc: 0.2457 - val_loss: 3.1951 - val_acc: 0.0890\n",
            "Epoch 19/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.1783 - acc: 0.2472 - val_loss: 3.0891 - val_acc: 0.1006\n",
            "Epoch 20/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 1.1639 - acc: 0.2517 - val_loss: 3.1190 - val_acc: 0.0986\n",
            "Epoch 21/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.1458 - acc: 0.2586 - val_loss: 3.0861 - val_acc: 0.1132\n",
            "Epoch 22/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 1.1274 - acc: 0.2784 - val_loss: 3.1062 - val_acc: 0.1220\n",
            "Epoch 23/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.1057 - acc: 0.2895 - val_loss: 3.1169 - val_acc: 0.1224\n",
            "Epoch 24/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 1.0901 - acc: 0.2946 - val_loss: 3.1006 - val_acc: 0.1273\n",
            "Epoch 25/100\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 1.0747 - acc: 0.2965 - val_loss: 3.0918 - val_acc: 0.1318\n",
            "Epoch 26/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 1.0625 - acc: 0.2979 - val_loss: 3.1251 - val_acc: 0.1246\n",
            "Epoch 27/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 1.0426 - acc: 0.3053 - val_loss: 3.1045 - val_acc: 0.1257\n",
            "Epoch 28/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 1.0262 - acc: 0.3096 - val_loss: 3.2426 - val_acc: 0.1128\n",
            "Epoch 29/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 1.0075 - acc: 0.3141 - val_loss: 3.2004 - val_acc: 0.1187\n",
            "Epoch 30/100\n",
            "55/55 [==============================] - 6s 102ms/step - loss: 0.9919 - acc: 0.3169 - val_loss: 3.1985 - val_acc: 0.1251\n",
            "Epoch 31/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.9757 - acc: 0.3232 - val_loss: 3.1503 - val_acc: 0.1317\n",
            "Epoch 32/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.9607 - acc: 0.3295 - val_loss: 3.1410 - val_acc: 0.1310\n",
            "Epoch 33/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.9408 - acc: 0.3343 - val_loss: 3.1937 - val_acc: 0.1285\n",
            "Epoch 34/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 0.9218 - acc: 0.3431 - val_loss: 3.1673 - val_acc: 0.1331\n",
            "Epoch 35/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.9030 - acc: 0.3515 - val_loss: 3.1812 - val_acc: 0.1327\n",
            "Epoch 36/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.8829 - acc: 0.3575 - val_loss: 3.1989 - val_acc: 0.1282\n",
            "Epoch 37/100\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 0.8656 - acc: 0.3641 - val_loss: 3.2485 - val_acc: 0.1267\n",
            "Epoch 38/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.8456 - acc: 0.3705 - val_loss: 3.2015 - val_acc: 0.1341\n",
            "Epoch 39/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.8242 - acc: 0.3780 - val_loss: 3.2250 - val_acc: 0.1312\n",
            "Epoch 40/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.8075 - acc: 0.3849 - val_loss: 3.3250 - val_acc: 0.1292\n",
            "Epoch 41/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.7921 - acc: 0.3941 - val_loss: 3.2709 - val_acc: 0.1299\n",
            "Epoch 42/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.7740 - acc: 0.3976 - val_loss: 3.2883 - val_acc: 0.1321\n",
            "Epoch 43/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.7584 - acc: 0.4043 - val_loss: 3.2712 - val_acc: 0.1334\n",
            "Epoch 44/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.7380 - acc: 0.4144 - val_loss: 3.2948 - val_acc: 0.1322\n",
            "Epoch 45/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.7218 - acc: 0.4187 - val_loss: 3.2580 - val_acc: 0.1376\n",
            "Epoch 46/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.7043 - acc: 0.4285 - val_loss: 3.3272 - val_acc: 0.1369\n",
            "Epoch 47/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.6869 - acc: 0.4352 - val_loss: 3.2808 - val_acc: 0.1483\n",
            "Epoch 48/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.6703 - acc: 0.4458 - val_loss: 3.3502 - val_acc: 0.1352\n",
            "Epoch 49/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.6532 - acc: 0.4534 - val_loss: 3.3292 - val_acc: 0.1368\n",
            "Epoch 50/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.6398 - acc: 0.4581 - val_loss: 3.4126 - val_acc: 0.1381\n",
            "Epoch 51/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.6192 - acc: 0.4736 - val_loss: 3.3727 - val_acc: 0.1399\n",
            "Epoch 52/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.6081 - acc: 0.4748 - val_loss: 3.3494 - val_acc: 0.1430\n",
            "Epoch 53/100\n",
            "55/55 [==============================] - 6s 102ms/step - loss: 0.5899 - acc: 0.4846 - val_loss: 3.4466 - val_acc: 0.1419\n",
            "Epoch 54/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.5764 - acc: 0.4956 - val_loss: 3.4331 - val_acc: 0.1412\n",
            "Epoch 55/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.5605 - acc: 0.5018 - val_loss: 3.4391 - val_acc: 0.1415\n",
            "Epoch 56/100\n",
            "55/55 [==============================] - 6s 105ms/step - loss: 0.5470 - acc: 0.5098 - val_loss: 3.5033 - val_acc: 0.1366\n",
            "Epoch 57/100\n",
            "55/55 [==============================] - 6s 103ms/step - loss: 0.5323 - acc: 0.5227 - val_loss: 3.4873 - val_acc: 0.1460\n",
            "Epoch 58/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 0.5181 - acc: 0.5309 - val_loss: 3.4626 - val_acc: 0.1424\n",
            "Epoch 59/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.5054 - acc: 0.5401 - val_loss: 3.4413 - val_acc: 0.1500\n",
            "Epoch 60/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.4944 - acc: 0.5433 - val_loss: 3.4847 - val_acc: 0.1473\n",
            "Epoch 61/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.4817 - acc: 0.5534 - val_loss: 3.5306 - val_acc: 0.1463\n",
            "Epoch 62/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.4661 - acc: 0.5643 - val_loss: 3.5000 - val_acc: 0.1486\n",
            "Epoch 63/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.4554 - acc: 0.5709 - val_loss: 3.5406 - val_acc: 0.1490\n",
            "Epoch 64/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.4406 - acc: 0.5814 - val_loss: 3.5633 - val_acc: 0.1471\n",
            "Epoch 65/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.4292 - acc: 0.5933 - val_loss: 3.5855 - val_acc: 0.1457\n",
            "Epoch 66/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.4219 - acc: 0.5973 - val_loss: 3.5850 - val_acc: 0.1410\n",
            "Epoch 67/100\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.4054 - acc: 0.6121 - val_loss: 3.5936 - val_acc: 0.1449\n",
            "Epoch 68/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.3913 - acc: 0.6244 - val_loss: 3.6549 - val_acc: 0.1407\n",
            "Epoch 69/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.3821 - acc: 0.6319 - val_loss: 3.6108 - val_acc: 0.1501\n",
            "Epoch 70/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.3724 - acc: 0.6423 - val_loss: 3.6906 - val_acc: 0.1453\n",
            "Epoch 71/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.3593 - acc: 0.6552 - val_loss: 3.7652 - val_acc: 0.1413\n",
            "Epoch 72/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.3494 - acc: 0.6603 - val_loss: 3.7309 - val_acc: 0.1448\n",
            "Epoch 73/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.3391 - acc: 0.6713 - val_loss: 3.6978 - val_acc: 0.1461\n",
            "Epoch 74/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.3308 - acc: 0.6763 - val_loss: 3.6837 - val_acc: 0.1459\n",
            "Epoch 75/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.3194 - acc: 0.6867 - val_loss: 3.7204 - val_acc: 0.1557\n",
            "Epoch 76/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 0.3117 - acc: 0.6928 - val_loss: 3.7038 - val_acc: 0.1510\n",
            "Epoch 77/100\n",
            "55/55 [==============================] - 6s 102ms/step - loss: 0.2996 - acc: 0.7060 - val_loss: 3.7778 - val_acc: 0.1445\n",
            "Epoch 78/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.2904 - acc: 0.7105 - val_loss: 3.8007 - val_acc: 0.1468\n",
            "Epoch 79/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2839 - acc: 0.7177 - val_loss: 3.7537 - val_acc: 0.1443\n",
            "Epoch 80/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2747 - acc: 0.7220 - val_loss: 3.7696 - val_acc: 0.1474\n",
            "Epoch 81/100\n",
            "55/55 [==============================] - 6s 102ms/step - loss: 0.2655 - acc: 0.7339 - val_loss: 3.8130 - val_acc: 0.1451\n",
            "Epoch 82/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2578 - acc: 0.7388 - val_loss: 3.8350 - val_acc: 0.1460\n",
            "Epoch 83/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 0.2519 - acc: 0.7430 - val_loss: 3.8520 - val_acc: 0.1510\n",
            "Epoch 84/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2447 - acc: 0.7475 - val_loss: 3.8604 - val_acc: 0.1496\n",
            "Epoch 85/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2398 - acc: 0.7546 - val_loss: 3.8512 - val_acc: 0.1472\n",
            "Epoch 86/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2347 - acc: 0.7547 - val_loss: 3.7979 - val_acc: 0.1523\n",
            "Epoch 87/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.2387 - acc: 0.7517 - val_loss: 3.9547 - val_acc: 0.1465\n",
            "Epoch 88/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.2214 - acc: 0.7658 - val_loss: 3.9148 - val_acc: 0.1488\n",
            "Epoch 89/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.2129 - acc: 0.7712 - val_loss: 3.8905 - val_acc: 0.1487\n",
            "Epoch 90/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 0.2058 - acc: 0.7781 - val_loss: 3.9775 - val_acc: 0.1462\n",
            "Epoch 91/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.2017 - acc: 0.7800 - val_loss: 3.9546 - val_acc: 0.1444\n",
            "Epoch 92/100\n",
            "55/55 [==============================] - 5s 99ms/step - loss: 0.1998 - acc: 0.7828 - val_loss: 3.9649 - val_acc: 0.1450\n",
            "Epoch 93/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.1981 - acc: 0.7807 - val_loss: 4.0077 - val_acc: 0.1474\n",
            "Epoch 94/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.1883 - acc: 0.7915 - val_loss: 4.0001 - val_acc: 0.1468\n",
            "Epoch 95/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.1850 - acc: 0.7885 - val_loss: 3.9875 - val_acc: 0.1468\n",
            "Epoch 96/100\n",
            "55/55 [==============================] - 6s 102ms/step - loss: 0.1785 - acc: 0.7981 - val_loss: 4.0244 - val_acc: 0.1468\n",
            "Epoch 97/100\n",
            "55/55 [==============================] - 6s 101ms/step - loss: 0.1741 - acc: 0.7995 - val_loss: 4.0085 - val_acc: 0.1458\n",
            "Epoch 98/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.1751 - acc: 0.7996 - val_loss: 4.0177 - val_acc: 0.1478\n",
            "Epoch 99/100\n",
            "55/55 [==============================] - 6s 100ms/step - loss: 0.1674 - acc: 0.8056 - val_loss: 4.0693 - val_acc: 0.1457\n",
            "Epoch 100/100\n",
            "55/55 [==============================] - 5s 100ms/step - loss: 0.1612 - acc: 0.8116 - val_loss: 4.0884 - val_acc: 0.1461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-sBZFsWhRML"
      },
      "source": [
        "##### Make predictions #####\n",
        "\n",
        "# The encoder will be stand-alone\n",
        "# From this we will get our initial decoder hidden state\n",
        "# i.e. h(1), ..., h(Tx)\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
        "\n",
        "# next we define a T=1 decoder model\n",
        "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "\n",
        "# no need to loop over attention steps this time because there is only one step\n",
        "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
        "\n",
        "# combine context with last word\n",
        "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
        "\n",
        "# lstm and final dense\n",
        "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
        "decoder_outputs = decoder_dense(o)\n",
        "\n",
        "\n",
        "# note: we don't really need the final stack and tranpose\n",
        "# because there's only 1 output\n",
        "# it is already of size N x D\n",
        "# no need to make it 1 x N x D --> N x 1 x D\n",
        "\n",
        "\n",
        "# create the model object\n",
        "decoder_model = Model(\n",
        "  inputs=[\n",
        "    decoder_inputs_single,\n",
        "    encoder_outputs_as_input,\n",
        "    initial_s, \n",
        "    initial_c\n",
        "  ],\n",
        "  outputs=[decoder_outputs, s, c]\n",
        ")\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  enc_out = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "\n",
        "  # [s, c] will be updated in each loop iteration\n",
        "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
        "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
        "\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
        "\n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(o.flatten())\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "  return ' '.join(output_sentence)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7d3xjTU_Csu"
      },
      "source": [
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(input_texts))\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_texts[i])\n",
        "  print('Predicted translation:', translation)\n",
        "  print('Actual translation:', target_texts[i])\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn20fFPF-_5Y",
        "outputId": "75013364-5252-4d07-eb50-ebce205a0198"
      },
      "source": [
        "string = 'I am not eating'\n",
        "string = string.lower()\n",
        "jam = []\n",
        "for i in string.split():\n",
        "  jam.append( word2idx_inputs[i] )\n",
        "jam = np.reshape( jam, (1, len(jam) ) )\n",
        "jam.shape\n",
        "jam_pad = pad_sequences(jam, maxlen=max_len_input)\n",
        "\n",
        "#translation = decode_sequence(input_seq)\n",
        "translation = decode_sequence(jam_pad)\n",
        "print('-')\n",
        "print('Input sentence:', string)\n",
        "print('Predicted translation:', translation)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: i am not eating\n",
            "Predicted translation: আমি ভাল নই।\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}